This is a folder containing all of my algorithms and data for IMC Prosperity 3

Apologies for the messy programming and lack of annotations, a lot of this was done under a time crunch. The competition lasted 15 days, consisted of 5 rounds, each 3 days long. I was learning a lot about trading and my programming was improving every day. In each round folder you will find various algos and some data files. In the research folder you will find notebooks for each round where I did analysis on the data. In the data folder you can find all order books, trades, and observations. You will probably notice as the rounds go on the programming getting somewhat cleaner at some point. If you're interested in trying any of these algorithms or using the data to write your own, I recommend using jmerle's Proserity 3 backtester and visualizer.

For a quick overview of the competition, each round (with the exception of round 5) introduced new products, each corresponding with different real financial instruments and issues that traders have to tackle. The products each come with their own inventory limits and your algorithm must be sufficiently fast to execute on the website submission (<900ms execution time). You get 3 days of historical data to analyze, backtest, and do whatever else you need to do with, and you can run your submission on 100k timestamps (10% of a 1M timestamp day) on the website before the submission is ran at the end of the round on a full new day. All of the products remain available to trade from each round they are introduced onwards.

Below is a brief synopsis of each round:


Round 1:

This is where everything started. The products traded here included RAINFOREST_RESIN, KELP, and SQUID_INK. The first two were the same from the tutorial round, but squid ink was new. I had developed market making strategies for both resin and kelp, but ended up switching to just using another team's strategy (They are called Linear Utility and have a great write up on Prosperity 2 from the previous year, they got 2nd place and were a great resource to reference). For squid ink, I initally had an algorithm that traded momentum based on a rolling z-score of returns threshold and otherwise did market making. In the inital website test data, this was really successful, but the day before submission, they changed the test data. (We later found out they accidentally gave us the first 100k timestamps of the data we were going to be running the submission on, which some people recognized and hardcoded their algos for, something that was later changed). After the data change, I realized my algo was overfit, and someone gave me a tip to look for patterns with the bots, so I tried my luck at kmeans clustering and found some interesting information: I was able to classify certain bots into different categories like big/small market maker, momentum trader, etc., but by the time I got that running, there were only 2 hours or so left until the submission. For round 1, I submitted an algorithm that did market making for resin and kelp, and tried to take advantage of a bot trend I found, but only lost shells on squid ink. After round 1, we were ranked around 550. It's important to note that resin and kelp netted ~30k per round consistently and it was expected any competitive team would attain that or better.


Round 2:

This round was really interesting. This is also when I realized the vast superiority of using a notebook to analyze data instead of my IDE ... crazy I know. There were 5 new products. PICNIC_BASKET1, PICNIC_BASKET2, JAMS, CROISSANTS, and DJEMBES. Basket 1 was comprised of 6 croissants, 4 jams, and 1 djembe. Basket 2 had 4 croissants and 2 jams. The initial approach was of course to try arbitrage with the underlying and synthetics. After some research, I found that not only was the spread between basket 1 and its synthetic mean reverting, but the spread between basket 1 and its synthetic AND basket 2 and its synthetic was mean reverting. This let me take the spread on just basket 1 and 2, freeing up the components to be traded on. The code here was kind of a mess because I used linear utility as a starting point and then heavily modified it and I probably should've just started from scratch which would have saved me a lot of time in the end. I didn't realize until round 3 that my implementation was slightly off, but it worked well and fixed the kinks later. After round 2, we were in 97th place. There were more data leaking issues with teams hardcoding stuff, and the admins finally came out and said anyone who continues to do so will be disqualified.


Round 3:

This round is where things started to get really tough. There were 6 new products. VOLCANIC_ROCK, VOLCANIC_ROCK_VOUCHER_9500, VOLCANIC_ROCK_VOUCHER_9750, VOLCANIC_ROCK_VOUCHER_10000, VOLCANIC_ROCK_VOUCHER_10250, and VOLCANIC_ROCK_VOUCHER_10500. The vouchers were all call options at their respectively named strike prices. I was brand new to options trading so I had to do some research on options themselves as well as the products we were now trading. I ended up writing an algorithm that I thought was going to be really good, but just about broke even. There were a few reasons for its underperformance intially that I will get into, and then later I will explain why I think it didn't work after those got fixed. The idea was to predict volatility regimes and bet on volatility while hedging underlying price movement. This is called vega-delta hedging. I made a function that used numpy vectorization to efficiently find an approximate optimization of maximum vega and minimum delta within the constraints of the product limits and the underlying product limit. The idea was that there is at any given time a maximum amount of vega that could be captured while hedging delta against not only the underlying, but other contracts as well. There were a lot of difficulties in implementing this bug free, one of which being the nature of the optimization I was using was very choppy in the way it would assign target positions. I later ended up using an exponential average to smooth this out, as well as the target position for the underlying. We got a hint towards the end of the round to look at the base implied volatility (something called the volatility smile). This was something I was already doing in a sense, by taking the average IV for each option across all the days, except the key part I was missing was putting them on a moneyness scale, a logarithmic scale with respect to strike, spot, and time that showed moneyness vs IV. I used this as criteria for entry point by using base IV to detect volatility regime and then comparing realized volatility vs implied volatility to enter contracts. I also filtered which options to trade based on the moneyness. This worked well enough, but was mostly breakeven. I was working on this algorithm until the very last minute until the submission at 6am. I accidentally deleted the line that hedged delta in the final submission, which caused us to lose a lot on the contracts, and we don't know if it would've been profitable or not. Additionally, the baskets ended up losing a lot of money too, because the mean reversion did not hold up for the 3rd day, something I should have probably put safeguards in place for, or maybe there was indication of that coming in the data, but my time was entirely consumed by options (as well as midterm exams). Besides this, I think the optimization function I implemented wasn't great. I heard from other groups that they traded solely on mean reversion to the volatility smile on only contracts close to the money and made significant profit. Additionally, I realized later I didn't adjust the time calculation for each timestamp, so the value the Black-Scholes function was spitting out was basically assuming everything happened at the start of the 3rd day, which was obviously not good. After round 3, we plummeted to 850th place or so, which was super disheartening, especially because it could've been avoided if I didn't accidentally delete the hedging line - at least we would've limited losses.


Round 4:

This round was pretty tough. There was 1 new product, the MAGNIFICENT_MACARONS. These were traded like a commodity from a neighboring island. A new mechanic was also introduced in this round, the observation conversion. This was kind of confusing and there wasn't great documentation on it, but it was essentially a way to trade on a different exchange with the other island. I didn't end up using it because I couldn't figure it out in time, but I know there was opportunity to calculate implied bids and asks and make some sort of arbitrage based on mispricings. Besides that, there were also things available in the observations that gave clues to the price movement of macarons, including sunlight levels, sugar price, import tariffs, export tariffs, and transport fees. There was also a holding fee of .1 shells per macaron per timestamp, which actually made things pretty difficult (you couldn't just hold it and wait). The macarons would often spike in price in correlation to the sunlightIndex, which created a good opportunity to make money on predicting the spikes. I ended up finding strong correlation between sunlightIndex and sugar price, which in turn drove big price movements in macarons. I ran various regressions, trying to predict the movement, and ultimately settled on trying to bet on reversion back to the regression line I made. I didn't realize when I submitted that I made a mistake in my code. The regression was supposed to calculate the price difference from the start of the day and trade on discrepancies, but I accidentally used .get instead of .setdefault for the starting price initalization, which made the price get set to current price at each run, meaning our algo went short at every timestamp. This lost about 30k shells unfortunately, and I will never know if the regression actually would've worked. The revised options algorithm also only just about broke even, although baskets made money again. Afterwards, I fixed this and made a revised algorithm that did a regression without the sunlight index to get a baseline and used a rolling z-score to check for unusual discrepancies in spread between actual price and baseline price. I think this would've worked well for R5, but I got scared after R4 and didn't end up submitting anything for macarons for round 5. After round 4, we were still unfortunately in a meager 844th place. --Edit: Looking back at this, there was most definitely lookahead bias anyway, so it may have been better that the regression did not get a chance...


Round 5:

This round got really interesting. For round 5, we got access to the names of the bots that have been taking the other end of our trades the entire time. Here I was able to revisit the work I was doing for round 1 on the bot detection. I spent a lot of time trying to figure out clustering and developed good average features for bots. I ended up giving up on the clustering because it wasn't getting me the results I was hoping for. I began to manually look for trade opportunities by graphing buys and sells by bot by product under certain conditions (i.e. above a certain order quantity, with a certain amount of bid volume, against certain bots, etc). I was able to find a couple opportunities that worked well in backtest. For example, there were two bots that if you followed all of their trades for baskets 1 and 2 yielded similar or slightly better results than basket arbitrage without the worry of finding a mean reversion point. There was another one that if you followed all the trades for each voucher filtered by moneyness made significant profit, and this was replicable to the underlying volcanic rock as well. I couldn't find anything for basket components unfortunately, so I didn't trade on those or on macarons at all out of fear of losing significant profit on those. I later found out that there was one bot (that I completely ignored because it only comprised around .2% of all trades, so I thought it was irrelevant), that made nearly perfect signals for croissants, all you had to do was follow her momentum and you'd make around 20k on the backtest. Next time I will have to do a better job of tracking change in price following trades. It did seem like there may have been some slippage on the inital website submission, but it's hard to tell on only 100k timestamps. Major slippage would be a disaster for these trade opportunities, but overall I really enjoyed round 5. Unfortunately, our algorithm didn't trade at all during round 5. There is an additional constraint we have to work around that I didn't mention yet. Since the algorithms are run on an AWS cloud, for some reason there is a chance on each iteration the variables can get lost, so, we are given the ability to store all of our data in a string in json format and decompress it at each iteration. This adds significant overhead, increasing the need for a lightweight, efficient algorithm. The efficiency wasn't the issue, but at round 5 I moved all of the code to a new file, starting over to make it sufficiently lightweight and readable. I forgot to add a line to deserialize the traderData string. So we made no trades and ended in 911th. I would say top 1000 out of 12000 teams with 3 dud rounds is not too bad.
